{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Rainforest Connecton Species Audio Detection Challenge](https://www.kaggle.com/competitions/rfcx-species-audio-detection/data) was a machine learning Kaggle competition to detect species from their audio recordings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aim\n",
    "\n",
    "Given audio files that include sounds from numerous species, the objective is to use machine learning to predict species in the audio clip. \n",
    "\n",
    "# Data Description\n",
    "Each audio recording is 5 mins long. There are total of 3958 audio recording files availabe for training. \n",
    "Each single recorded audio contains signals from multiple species which are labelled by time-stamps `t_min` and `t_max`, where `t_min` indicates the start time the a particular species was heard and  `t_max` the time until the species was heard. Also provided are frequency range(`f_min`, `f_max`) of that particular siganl. \n",
    "\n",
    "\n",
    "Training data also includes false positive label occurrences to assist with training.\n",
    "\n",
    "Files: \n",
    "\n",
    "- `train_tp.csv` - training data of true positive species labels, with corresponding time localization.\n",
    "- `train_fp.csv` - training data of false positives species labels, with corresponding time localization\n",
    "- `train/` - the training audio files\n",
    "- `test/` - the test audio files; the task is to predict the species found in each audio file\n",
    "\n",
    "The meta file `train_tp.csv` contains the following columns:\n",
    "\n",
    "- `recording_id` - unique identifier for recording\n",
    "- `species_id` - unique identifier for species\n",
    "- `songtype_id` - unique identifier for songtype\n",
    "- `t_min` - start second of annotated signal\n",
    "- `f_min` - lower frequency of annotated signal\n",
    "- `t_max` - end second of annotated signal\n",
    "- `f_max` - upper frequency of annotated signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach:\n",
    "\n",
    "Past studies showed that rather than directly use the audio signals for species detection, instead using images of mel-spectrogram of the audio signals can be  used train deep learning models to perform species identification. The approach used in this notebook is the same. \n",
    "\n",
    "\n",
    "Since EDA indicates that each signal is approx. `2.64 sec` long. The plan is to chop each audio recordings (each `5 min` long) into `10 sec` long clips. \n",
    "Then plot the Mel-spectrogram, and save it as images which could be then be used to train deep learning models. There are a total of 24 different species that haven been labelled.  \n",
    "\n",
    "This notebook illustrates how the audio recordings can be preprocessed to generate images of mel-spectrogram, and  how these images can used to detect species using a deep-learning model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-10-08T17:33:06.031822Z",
     "iopub.status.busy": "2022-10-08T17:33:06.031447Z",
     "iopub.status.idle": "2022-10-08T17:33:19.157071Z",
     "shell.execute_reply": "2022-10-08T17:33:19.155881Z",
     "shell.execute_reply.started": "2022-10-08T17:33:06.031785Z"
    },
    "papermill": {
     "duration": 4.490022,
     "end_time": "2020-12-22T19:44:25.45993",
     "exception": false,
     "start_time": "2020-12-22T19:44:20.969908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "# Optimizer\n",
    "import torch.optim as optim\n",
    "# Libraries for handling dataset\n",
    "from torch.utils.data import Dataset as BaseDataset, DataLoader\n",
    "\n",
    "# LR Scheduler\n",
    "from transformers import get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup\n",
    "\n",
    "from torchvision import transforms \n",
    "\n",
    "# Load pretrained computer-vision model\n",
    "from torchvision.models.resnet import resnet18, resnet34, resnet50\n",
    "\n",
    "# Libraries for handling audio files\n",
    "import librosa as lb  \n",
    "\n",
    "# Libraries for plotting Spectrograms from audio signals\n",
    "# from audiomentations import *\n",
    "\n",
    "# Libraries for handling images\n",
    "import skimage.io\n",
    "from skimage import transform\n",
    "\n",
    "print(lb.__version__)\n",
    "#print(python.__version__)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-08T17:33:19.158871Z",
     "iopub.status.busy": "2022-10-08T17:33:19.158553Z",
     "iopub.status.idle": "2022-10-08T17:33:19.293663Z",
     "shell.execute_reply": "2022-10-08T17:33:19.292290Z",
     "shell.execute_reply.started": "2022-10-08T17:33:19.158840Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00204008d' '003b04435' '005f1f9a5' ... 'ffebe7313' 'fff163132'\n",
      " 'fffb79246'] # Number of recordings =  3958\n",
      "[21  8  4 22 23 10  2  1 11 19 20 17  9 15 16  6  5  3 18 12  0 13 14  7] # Number of species =  24\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recording_id</th>\n",
       "      <th>species_id</th>\n",
       "      <th>songtype_id</th>\n",
       "      <th>t_min</th>\n",
       "      <th>f_min</th>\n",
       "      <th>t_max</th>\n",
       "      <th>f_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00204008d</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>13.8400</td>\n",
       "      <td>3281.25</td>\n",
       "      <td>14.9333</td>\n",
       "      <td>4125.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00204008d</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>24.4960</td>\n",
       "      <td>3750.00</td>\n",
       "      <td>28.6187</td>\n",
       "      <td>5531.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00204008d</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0027</td>\n",
       "      <td>2343.75</td>\n",
       "      <td>16.8587</td>\n",
       "      <td>4218.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  recording_id  species_id  songtype_id    t_min    f_min    t_max    f_max\n",
       "0    00204008d          21            1  13.8400  3281.25  14.9333  4125.00\n",
       "1    00204008d           8            1  24.4960  3750.00  28.6187  5531.25\n",
       "2    00204008d           4            1  15.0027  2343.75  16.8587  4218.75"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recording_id</th>\n",
       "      <th>species_id</th>\n",
       "      <th>songtype_id</th>\n",
       "      <th>t_min</th>\n",
       "      <th>f_min</th>\n",
       "      <th>t_max</th>\n",
       "      <th>f_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003b04435</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>43.2533</td>\n",
       "      <td>10687.5000</td>\n",
       "      <td>44.8587</td>\n",
       "      <td>13687.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003b04435</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>9.1254</td>\n",
       "      <td>7235.1562</td>\n",
       "      <td>15.2091</td>\n",
       "      <td>11283.3984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  recording_id  species_id  songtype_id    t_min       f_min    t_max  \\\n",
       "3    003b04435          22            1  43.2533  10687.5000  44.8587   \n",
       "4    003b04435          23            1   9.1254   7235.1562  15.2091   \n",
       "\n",
       "        f_max  \n",
       "3  13687.5000  \n",
       "4  11283.3984  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recording_id</th>\n",
       "      <th>species_id</th>\n",
       "      <th>songtype_id</th>\n",
       "      <th>t_min</th>\n",
       "      <th>f_min</th>\n",
       "      <th>t_max</th>\n",
       "      <th>f_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>005f1f9a5</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>49.3598</td>\n",
       "      <td>947.4609</td>\n",
       "      <td>51.8037</td>\n",
       "      <td>10852.7344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  recording_id  species_id  songtype_id    t_min     f_min    t_max  \\\n",
       "5    005f1f9a5          10            1  49.3598  947.4609  51.8037   \n",
       "\n",
       "        f_max  \n",
       "5  10852.7344  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>species_id</th>\n",
       "      <td>7781.0</td>\n",
       "      <td>12.138671</td>\n",
       "      <td>7.068808</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>12.0000</td>\n",
       "      <td>18.0000</td>\n",
       "      <td>23.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>songtype_id</th>\n",
       "      <td>7781.0</td>\n",
       "      <td>1.346999</td>\n",
       "      <td>0.959535</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>4.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_min</th>\n",
       "      <td>7781.0</td>\n",
       "      <td>28.627830</td>\n",
       "      <td>17.461603</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>12.9493</td>\n",
       "      <td>28.8800</td>\n",
       "      <td>44.0657</td>\n",
       "      <td>59.3013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_min</th>\n",
       "      <td>7781.0</td>\n",
       "      <td>2827.996428</td>\n",
       "      <td>2515.604420</td>\n",
       "      <td>93.7500</td>\n",
       "      <td>947.4609</td>\n",
       "      <td>2343.7500</td>\n",
       "      <td>3843.7500</td>\n",
       "      <td>10687.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_max</th>\n",
       "      <td>7781.0</td>\n",
       "      <td>31.267911</td>\n",
       "      <td>17.496989</td>\n",
       "      <td>0.7680</td>\n",
       "      <td>15.7280</td>\n",
       "      <td>31.5413</td>\n",
       "      <td>46.7893</td>\n",
       "      <td>59.9947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_max</th>\n",
       "      <td>7781.0</td>\n",
       "      <td>6074.830415</td>\n",
       "      <td>3386.040304</td>\n",
       "      <td>843.7500</td>\n",
       "      <td>3937.5000</td>\n",
       "      <td>5250.0000</td>\n",
       "      <td>7500.0000</td>\n",
       "      <td>13687.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>signal_duration</th>\n",
       "      <td>7781.0</td>\n",
       "      <td>2.640081</td>\n",
       "      <td>1.820595</td>\n",
       "      <td>0.2720</td>\n",
       "      <td>1.2906</td>\n",
       "      <td>2.1440</td>\n",
       "      <td>3.3440</td>\n",
       "      <td>7.9239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  count         mean          std       min        25%  \\\n",
       "species_id       7781.0    12.138671     7.068808    0.0000     6.0000   \n",
       "songtype_id      7781.0     1.346999     0.959535    1.0000     1.0000   \n",
       "t_min            7781.0    28.627830    17.461603    0.0107    12.9493   \n",
       "f_min            7781.0  2827.996428  2515.604420   93.7500   947.4609   \n",
       "t_max            7781.0    31.267911    17.496989    0.7680    15.7280   \n",
       "f_max            7781.0  6074.830415  3386.040304  843.7500  3937.5000   \n",
       "signal_duration  7781.0     2.640081     1.820595    0.2720     1.2906   \n",
       "\n",
       "                       50%        75%         max  \n",
       "species_id         12.0000    18.0000     23.0000  \n",
       "songtype_id         1.0000     1.0000      4.0000  \n",
       "t_min              28.8800    44.0657     59.3013  \n",
       "f_min            2343.7500  3843.7500  10687.5000  \n",
       "t_max              31.5413    46.7893     59.9947  \n",
       "f_max            5250.0000  7500.0000  13687.5000  \n",
       "signal_duration     2.1440     3.3440      7.9239  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg signal duration (sec) 2.64008140341857\n",
      "Number of species = 24\n"
     ]
    }
   ],
   "source": [
    "# Review rfcx-species-detection challenge \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "path = '../input/rfcx-species-audio-detection'\n",
    "train_csv = pd.read_csv(path + '/train_fp.csv')\n",
    "\n",
    "display(train_csv.head())\n",
    "\n",
    "rec_ids = train_csv.recording_id.unique()\n",
    "print(rec_ids, '# Number of recordings = ', len(rec_ids))\n",
    "\n",
    "species_ids = train_csv.species_id.unique()\n",
    "print(species_ids, '# Number of species = ', len(species_ids))\n",
    "\n",
    "# Compute average duration of signal\n",
    "train_csv['signal_duration'] = train_csv.t_max - train_csv.t_min\n",
    "display(train_csv.describe().transpose())\n",
    "\n",
    "avg_signal_duration = train_csv.signal_duration.mean()\n",
    "print('Avg signal duration (sec)', avg_signal_duration)\n",
    "\n",
    "print('Number of species =', train_csv.species_id.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class `Preprocessing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2022-10-08T17:33:19.295782Z",
     "iopub.status.busy": "2022-10-08T17:33:19.295434Z",
     "iopub.status.idle": "2022-10-08T17:33:19.316985Z",
     "shell.execute_reply": "2022-10-08T17:33:19.315714Z",
     "shell.execute_reply.started": "2022-10-08T17:33:19.295749Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.049105,
     "end_time": "2020-12-22T19:44:25.598304",
     "exception": false,
     "start_time": "2020-12-22T19:44:25.549199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    \n",
    "    sample_length = 5 \n",
    "    sampling_rate = 48000\n",
    "    image_size = (224, 224)\n",
    "    AUDIO_FOLDER = '../input/rfcx-species-audio-detection'\n",
    "   \n",
    "    def __init__(self, meta_data):\n",
    "        self.META_FILE = meta_data\n",
    "        \n",
    "    def train_data(self, OUTPUT_IMAGE_FOLDER = 'TRAIN_IMAGES'):\n",
    "        '''\n",
    "        PREPROCESSING: AUDIO SIGNAL --> IMAGE.\n",
    "        Preprocessing steps: \n",
    "        1. Uses package librosa to load the audio file as a numpy array.\n",
    "        2. Select the sample duration from the audio-file.\n",
    "        3. Plot the Mel-Spectrogram of the sample.\n",
    "        4. Convert the MEL-spectrogram to image of desired size.\n",
    "        5. Save the image.\n",
    "        '''\n",
    "        start_time = time.time()\n",
    "\n",
    "        for idx in range(len(self.META_FILE)):\n",
    "            record_id = self.META_FILE['recording_id'].iloc[idx] \n",
    "            label = self.META_FILE['species_id'].iloc[idx]\n",
    "\n",
    "            # Use librosa to load the audio file as a numpy array...\n",
    "            audio_filepath = f'{self.AUDIO_FOLDER}/train/{record_id}.flac'\n",
    "            \n",
    "            audio_data, _ = lb.load(audio_filepath, sr = self.sampling_rate) # sr -  sampling rate\n",
    "\n",
    "            # Duration of signal is (t_max - t_min). \n",
    "            t_min = self.META_FILE['t_min'].iloc[idx] \n",
    "            t_max = self.META_FILE['t_max'].iloc[idx]\n",
    "\n",
    "            # Select sample start time such that it contains the signal.\n",
    "            sample_start_time =  (t_min + t_max) * 0.5 - self.sample_length * 0.5\n",
    "            sample_end_time = sample_start_time + self.sample_length\n",
    "\n",
    "            if sample_start_time < 0:\n",
    "                sample_start_time = 0.\n",
    "                sample_end_time = sample_start_time + self.sample_length\n",
    "\n",
    "            if sample_end_time > (len(audio_data) / self.sampling_rate):\n",
    "                sample_end_time = len(audio_data) / self.sampling_rate\n",
    "                sample_start_time = sample_end_time - self.sample_length\n",
    "\n",
    "            start_idx = int(sample_start_time * self.sampling_rate)\n",
    "            end_idx = int(sample_end_time * self.sampling_rate)\n",
    "\n",
    "            sample = audio_data[start_idx:end_idx]\n",
    "            S = lb.feature.melspectrogram(sample, self.sampling_rate,  \n",
    "                n_fft = 2048,                   # WINDOW LENGHT             \n",
    "                hop_length = 512,               # WINDOW LENGHT // 4  \n",
    "                n_mels = 128, \n",
    "                fmin = 0,                    \n",
    "                fmax = self.sampling_rate // 2 # HALF THE SAMPLING RATE     \n",
    "            )\n",
    "            # Convert ot power\n",
    "            D = lb.power_to_db(S, ref=np.max)\n",
    "\n",
    "            # Resize the MEL-Spectrogram to image of (224x224)\n",
    "            D = skimage.transform.resize(D, self.image_size)\n",
    "\n",
    "            # Normalize the image\n",
    "            D = D - D.min()\n",
    "            D = D / (D.max() + 1.0e-10)        \n",
    "\n",
    "            # Save image as np.array...\n",
    "            if not Path(OUTPUT_IMAGE_FOLDER).exists():\n",
    "                os.mkdir(OUTPUT_IMAGE_FOLDER)          \n",
    "            path_train_images = Path(OUTPUT_IMAGE_FOLDER) / ('RID{}_SID{:0>2d}'.format(record_id, label))  \n",
    "            np.save(path_train_images, D)\n",
    "            \n",
    "        print('Number of audio files for training:', len(self.META_FILE))\n",
    "        print(f'TRAINING IMAGES SAVED TO FOLDER: {OUTPUT_IMAGE_FOLDER}')\n",
    "        print('Number of training images: ', len(os.listdir(OUTPUT_IMAGE_FOLDER)))\n",
    "        print('Image Size:', self.image_size)\n",
    "        print('Processing time {:.2f} seconds'.format(time.time() - start_time ))\n",
    "\n",
    "    def val_data(self, OUTPUT_IMAGE_FOLDER = 'VAL_IMAGES'):\n",
    "        start_time = time.time()\n",
    "        for idx in range(len(self.META_FILE)):    \n",
    "                record_id = self.META_FILE['recording_id'].iloc[idx] \n",
    "                label = self.META_FILE['species_id'].iloc[idx]\n",
    "                audio_file  = f'{self.AUDIO_FOLDER}/train/{record_id}.flac'\n",
    "                audio_data, _ = lb.load(audio_file, sr = self.sampling_rate) \n",
    "                n_samples = len(audio_data) // (self.sample_length * self.sampling_rate)\n",
    "\n",
    "                start_idx = 0\n",
    "                end_idx = start_idx + (self.sample_length * self.sampling_rate)\n",
    "\n",
    "                if len(audio_data) < self.sample_length * self.sampling_rate:\n",
    "                    n_samples = 1\n",
    "                    end_idx = len(audio_data) \n",
    "\n",
    "                for j in range(n_samples):\n",
    "                    S = lb.feature.melspectrogram(audio_data[start_idx:end_idx], self.sampling_rate, n_fft = 2048, hop_length = 2048 // 4, n_mels = 128,\n",
    "                                                  fmin = 0, fmax = self.sampling_rate // 2)\n",
    "                    D = lb.power_to_db(S, ref= np.max)\n",
    "                    # RESIZE\n",
    "                    D = skimage.transform.resize(D, (224, 224))\n",
    "                    # Normalize\n",
    "                    D = D - D.min()\n",
    "                    D = D / (D.max() + 1.0e-10)   \n",
    "                    if not Path(OUTPUT_IMAGE_FOLDER).exists():\n",
    "                        os.mkdir(OUTPUT_IMAGE_FOLDER)      \n",
    "                    path_val_images = Path(OUTPUT_IMAGE_FOLDER) / ('RID{}_SID{:0>2d}_CLIP{:0>2d}'.format(record_id, label, j))  \n",
    "                    np.save(path_val_images, D)\n",
    "                    # Update the `start_idx` and `end_idx`\n",
    "                    start_idx = end_idx\n",
    "                    end_idx = start_idx + self.sample_length * self.sampling_rate\n",
    "        print('') \n",
    "        print('Number of audio files for validation:', len(self.META_FILE))\n",
    "        print(f'VALIDATION IMAGES SAVED TO FOLDER: {OUTPUT_IMAGE_FOLDER}')\n",
    "        print('Number of validating images: ', len(os.listdir(OUTPUT_IMAGE_FOLDER)))\n",
    "        print('Image Size:', self.image_size)\n",
    "        print('Processing time {:.2f} seconds'.format(time.time() - start_time ))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric: label-weighted label rankded average precision `(lwlrap)` \n",
    "1. https://scikit-learn.org/stable/modules/model_evaluation.html#label-ranking-average-precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-10-09T19:38:15.283469Z",
     "iopub.status.busy": "2022-10-09T19:38:15.283077Z",
     "iopub.status.idle": "2022-10-09T19:38:15.294968Z",
     "shell.execute_reply": "2022-10-09T19:38:15.293697Z",
     "shell.execute_reply.started": "2022-10-09T19:38:15.283437Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _one_sample_positive_class_precisions(scores, truth, TRAIN, threshold):\n",
    "    num_classes = scores.shape[0]\n",
    "    \n",
    "    if not TRAIN: threshold = 0\n",
    "    pos_class_indices = np.flatnonzero(truth > threshold)\n",
    "    if not len(pos_class_indices):\n",
    "        return pos_class_indices, np.zeros(0)\n",
    "\n",
    "    retrieved_classes = np.argsort(scores)[::-1]\n",
    "    class_rankings = np.zeros(num_classes, dtype=np.int32)\n",
    "    class_rankings[retrieved_classes] = range(num_classes)\n",
    "    retrieved_class_true = np.zeros(num_classes, dtype=np.bool)\n",
    "    retrieved_class_true[class_rankings[pos_class_indices]] = True\n",
    "    retrieved_cumulative_hits = np.cumsum(retrieved_class_true)\n",
    "    precision_at_hits = (\n",
    "            retrieved_cumulative_hits[class_rankings[pos_class_indices]] /\n",
    "            (1 + class_rankings[pos_class_indices].astype(np.float32)))\n",
    "    return pos_class_indices, precision_at_hits\n",
    "\n",
    "def lwlrap(truth, scores, ALPHA_LABEL = 0, TRAIN=True):\n",
    "    \"\"\" \n",
    "    Label-Weighted Label Rated Average Precision.\n",
    "    Reference:\n",
    "    1. https://www.kaggle.com/c/rfcx-species-audio-detection/discussion/198418\n",
    "    2. https://scikit-learn.org/stable/modules/model_evaluation.html#label-ranking-average-precision\n",
    "    \"\"\"\n",
    "    assert truth.shape == scores.shape\n",
    "    \n",
    "    num_samples, num_classes = scores.shape\n",
    "\n",
    "    if TRAIN:\n",
    "        threshold = (ALPHA_LABEL / num_classes)\n",
    "    else:\n",
    "        threshold = 0\n",
    "    \n",
    "    precisions_for_samples_by_classes = np.zeros((num_samples, num_classes), dtype=np.float32)\n",
    "    \n",
    "    for sample_num in range(num_samples):\n",
    "        pos_class_indices, precision_at_hits = _one_sample_positive_class_precisions(\n",
    "            scores[sample_num, :], truth[sample_num, :], TRAIN, threshold\n",
    "        )   \n",
    "        precisions_for_samples_by_classes[sample_num, pos_class_indices] = precision_at_hits\n",
    "    \n",
    "    \n",
    "    #labels_per_class = np.sum(truth > 0, axis=0)\n",
    "    labels_per_class = np.sum(truth > threshold, axis=0)\n",
    "\n",
    "    #weight_per_class = labels_per_class / float(np.sum(labels_per_class))\n",
    "    weight_per_class = labels_per_class / np.sum(labels_per_class).astype(np.float32)\n",
    "\n",
    "    per_class_lwlrap = (np.sum(precisions_for_samples_by_classes, axis=0) /\n",
    "                np.maximum(1, labels_per_class))\n",
    "    \n",
    "    return per_class_lwlrap, weight_per_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `class Train_Validate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-08T19:40:52.306367Z",
     "iopub.status.busy": "2022-10-08T19:40:52.306003Z",
     "iopub.status.idle": "2022-10-08T19:40:52.329756Z",
     "shell.execute_reply": "2022-10-08T19:40:52.328245Z",
     "shell.execute_reply.started": "2022-10-08T19:40:52.306331Z"
    }
   },
   "outputs": [],
   "source": [
    "class Train_Validate(nn.Module):\n",
    "    \n",
    "    device =  torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # Training Parameters...\n",
    "    CHECKPOINT_EPOCH = 5\n",
    "    EPOCHS = 5\n",
    "    LR = 1.0e-03             # Initial Learing Rate\n",
    "    MODEL_NAME = 'resnet34'\n",
    "    MODEL_STATE_FOLDER = 'MODEL-STATE'\n",
    "    NUM_CLASSES = 24    \n",
    "    # Cosine Scheduler with Warmup start\n",
    "    num_warmup_steps = 5\n",
    "    num_training_steps = 30 \n",
    "\n",
    "        \n",
    "    def __init__(self, fold=0):\n",
    "        super().__init__()\n",
    "        self.fold = fold\n",
    "        self.model = self.get_model(self.MODEL_NAME)\n",
    "        self.model.to(self.device)\n",
    "          \n",
    "        self.criterion = nn.BCEWithLogitsLoss().to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr = self.LR)  \n",
    "        self.scheduler = get_cosine_schedule_with_warmup(self.optimizer, self.num_warmup_steps, self.num_training_steps)\n",
    "        \n",
    "    def get_model(self, MODEL_NAME, PRETRAINED=True):\n",
    "        if MODEL_NAME == 'resnet34':\n",
    "            model = resnet34(pretrained=PRETRAINED)  \n",
    "            del model.fc\n",
    "            model.fc = nn.Linear(in_features=512, out_features=self.NUM_CLASSES, bias=True)\n",
    "        return model\n",
    "\n",
    "    def fit(self, train_dl, val_dl):\n",
    "        total_train_loss = []\n",
    "        total_val_loss = []\n",
    "        total_train_score = []\n",
    "        total_val_score = []\n",
    "        for epoch in range(self.EPOCHS):\n",
    "            train_loss, train_score = self.train_fn(train_dl)            \n",
    "            total_train_loss.append(train_loss)\n",
    "            total_train_score.append(train_score)\n",
    "            #print('TRAIN: Epoch: {:0>2d};  Loss = {:.3f}; Precision = {:.3f}'.format(epoch, train_loss, train_score))\n",
    "            val_loss, val_score = self.val_fn(val_dl)\n",
    "            total_val_loss.append(val_loss)\n",
    "            total_val_score.append(val_score)  \n",
    "            print('Epoch: {:0>2d}; Train: Loss = {:.3f}; Precision = {:.3f}; Val: Loss = {:.3f}; Precision = {:.3f}'.format(\n",
    "                epoch, train_loss, train_score, val_loss, val_score))            \n",
    "            # Call the step fn on the scheduler to update the lr...\n",
    "            self.scheduler.step()\n",
    "        # Plot the results...\n",
    "        self.visualize_results(total_train_loss, total_train_score, total_val_loss, total_val_score)\n",
    "            \n",
    "    def train_fn(self, train_dl):\n",
    "        self.model.train()\n",
    "        losses = []\n",
    "        outputs = []\n",
    "        labels = []\n",
    "        for input, label in train_dl:\n",
    "            output = self.model(input.to(self.device))                  \n",
    "            loss = self.criterion(output, label.to(self.device))\n",
    "            self.optimizer.zero_grad()               \n",
    "            loss.backward()                \n",
    "            self.optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "            # Saving model ouput and labels to check Precision.....\n",
    "            outputs.append(torch.sigmoid(output).cpu().detach().numpy()) \n",
    "            labels.append(label.cpu().detach().numpy())  \n",
    "        # Compute Precision ...\n",
    "        score = self.get_precision(labels, outputs)\n",
    "        return np.mean(losses), score\n",
    "    \n",
    "    def val_fn(self, val_dl):\n",
    "        self.model.eval()\n",
    "        labels = []\n",
    "        probs = []\n",
    "        losses = []\n",
    "        with torch.no_grad():\n",
    "            for image, label in val_dl:\n",
    "                # Make prediction...\n",
    "                pred = self.model(image.to(self.device))\n",
    "                loss = self.criterion(pred, label.to(self.device))\n",
    "                losses.append(loss.item())\n",
    "                # Get probabilities...\n",
    "                prob = torch.sigmoid(pred)\n",
    "                max_prob, _ = torch.max(prob, axis = 0)                    \n",
    "                probs.append(max_prob.cpu().detach().numpy())\n",
    "                labels.append(label[0, :].cpu().detach().numpy())   \n",
    "        # Compute Precision ...        \n",
    "        score = self.get_precision(labels, probs)\n",
    "        return np.mean(losses), score\n",
    "    \n",
    "    def get_precision(self, labels, probs, TRAIN=True):\n",
    "        y_score = np.vstack(probs)\n",
    "        y_true = np.vstack(labels)\n",
    "        score_class, weight = lwlrap(y_true, y_score, TRAIN)\n",
    "        score_v = (score_class * weight).sum()\n",
    "        return score_v\n",
    "    \n",
    "    def model_checkpoint(self, epoch):\n",
    "        if (epoch+1) % self.CHECKPOINT_EPOCH == 0:\n",
    "            if not Path(self.MODEL_STATE_FOLDER).exists():\n",
    "                os.mkdir(self.MODEL_STATE_FOLDER)\n",
    "            path_model_checkpoint = Path(self.MODEL_STATE_FOLDER) / 'rfcx-{}-EPOCH{}-FOLD{}'.format(MODEL_NAME, epoch+1, self.fold)\n",
    "            torch.save(\n",
    "                {\n",
    "                'spectrogram_params': params,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'opt_state_dict': optimizer.state_dict(),\n",
    "                'epoch': (epoch+1),\n",
    "                'loss': loss\n",
    "                }, \n",
    "                path_model_checkpoint\n",
    "            )   \n",
    "            \n",
    "    def visualize_results(self, train_loss, train_score, val_loss, val_score):\n",
    "        fig, axes = plt.subplots(1, 2, sharex=True, figsize=(6, 4))\n",
    "        ax = np.ravel(axes)\n",
    "        ax[0].plot(train_loss, 'r-', val_loss, 'b-')\n",
    "        ax[0].set_ylim([0, 1.])\n",
    "        ax[0].set_ylabel('Loss')\n",
    "        ax[0].legend(['train', 'val'])\n",
    "        ax[1].plot(train_score, 'r-', val_score, 'b-')\n",
    "        ax[1].set_ylim([0, 1.])    \n",
    "        ax[1].set_ylabel('Precision')\n",
    "        ax[1].set_xlabel('Epoch')\n",
    "        ax[1].legend(['train', 'val'])\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-10-08T19:09:53.059983Z",
     "iopub.status.busy": "2022-10-08T19:09:53.059636Z",
     "iopub.status.idle": "2022-10-08T19:09:53.070158Z",
     "shell.execute_reply": "2022-10-08T19:09:53.069432Z",
     "shell.execute_reply.started": "2022-10-08T19:09:53.059952Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.049105,
     "end_time": "2020-12-22T19:44:25.598304",
     "exception": false,
     "start_time": "2020-12-22T19:44:25.549199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dataset(BaseDataset):\n",
    "    def __init__(self, TRAIN=True):\n",
    "        super().__init__()\n",
    "        self.train = TRAIN\n",
    "        if self.train:\n",
    "            self.path_images = 'TRAIN_IMAGES'\n",
    "            self.images = sorted(os.listdir(self.path_images))\n",
    "        else:\n",
    "            self.path_images = 'VAL_IMAGES'\n",
    "            self.images = sorted(os.listdir(self.path_images))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images) \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        if self.train:\n",
    "            if len(image) <= 22:\n",
    "                label = int(image[16:18])\n",
    "                onehot_label = np.zeros(24, dtype = np.float32)\n",
    "                onehot_label[label] = 1.0\n",
    "            else:\n",
    "                label1 = int(file[16:18])\n",
    "                label2 = int(file[18:20])\n",
    "                label3 = int(file[20:22])\n",
    "\n",
    "                onehot_label = np.zeros(24, dtype = np.float32)\n",
    "                onehot_label[label1] = 1.0\n",
    "                onehot_label[label2] = 1.0\n",
    "                onehot_label[label3] = 1.0\n",
    "        else:\n",
    "            label = int(image[16:18])\n",
    "            onehot_label = np.zeros(24, dtype = np.float32)\n",
    "            onehot_label[label] = 1.0\n",
    "\n",
    "        # Load the spectrogram from the folder...\n",
    "        X = np.load(self.path_images + '/' + image)             \n",
    "        # Convert mono to color with first dimension as channel.\n",
    "        image_3d = np.stack([X, X, X]) #, dtype=np.float32)         \n",
    "        return image_3d.astype(np.float32), onehot_label\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into 80\\% `training` and 20% `validation` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-10-08T19:10:00.191712Z",
     "iopub.status.busy": "2022-10-08T19:10:00.191341Z",
     "iopub.status.idle": "2022-10-08T19:10:26.234525Z",
     "shell.execute_reply": "2022-10-08T19:10:26.233005Z",
     "shell.execute_reply.started": "2022-10-08T19:10:00.191657Z"
    },
    "papermill": {
     "duration": 2497.803043,
     "end_time": "2020-12-22T20:26:04.409666",
     "exception": false,
     "start_time": "2020-12-22T19:44:26.606623",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:672: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of audio files for training: 80\n",
      "TRAINING IMAGES SAVED TO FOLDER: TRAIN_IMAGES\n",
      "Number of training images:  78\n",
      "Image Size: (224, 224)\n",
      "Processing time 8.42 seconds\n",
      "\n",
      "Number of audio files for validation: 20\n",
      "VALIDATION IMAGES SAVED TO FOLDER: VAL_IMAGES\n",
      "Number of validating images:  240\n",
      "Image Size: (224, 224)\n",
      "Processing time 17.60 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AUDIO_FOLDER = '../input/rfcx-species-audio-detection'\n",
    "\n",
    "# Load the meta-file\n",
    "META_FILE = pd.read_csv(f'{AUDIO_FOLDER}/train_tp.csv')\n",
    "\n",
    "# We only use with a small fraction of the available data (1000 audio recordings) for now ...\n",
    "META_FILE = META_FILE.iloc[:1000]\n",
    "\n",
    "# Split the data into train and validation using StratifiedKFold...\n",
    "from sklearn.model_selection import StratifiedKFold as skf\n",
    "kfold = skf(n_splits=5, shuffle=True, random_state=32)\n",
    "record_ids = META_FILE['recording_id'].tolist()\n",
    "species_ids = META_FILE['species_id'].tolist()\n",
    "\n",
    "for fold, (train_indices, val_indices) in enumerate(\n",
    "    kfold.split(record_ids, species_ids)\n",
    "):        \n",
    "    print('FOLD =', fold)\n",
    "    # Only using fold 0...\n",
    "    if fold == 0:\n",
    "        # Load Training dataset...\n",
    "        TRAIN_META_FILE = META_FILE.iloc[train_indices] \n",
    "        Preprocessing(TRAIN_META_FILE).train_data()\n",
    "        train_ds = Dataset(TRAIN=True)\n",
    "        train_dl = DataLoader(train_ds, batch_size = 16, shuffle = True, num_workers = 0)\n",
    "\n",
    "        # Load Validation data ...\n",
    "        VAL_META_FILE = META_FILE.iloc[val_indices]\n",
    "        Preprocessing(VAL_META_FILE).val_data()                       \n",
    "        val_ds = Dataset(TRAIN=False) \n",
    "        val_dl = DataLoader(val_ds, batch_size=8, shuffle = False, drop_last = True)\n",
    "                \n",
    "        # Perform training and validation...\n",
    "        Train_Validate(fold).fit(train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concluding Remarks\n",
    "\n",
    "Further improvement in the result can be achieved by adding 1. `image augmentations` 2. `mixup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2022-10-08T17:33:55.481127Z",
     "iopub.status.idle": "2022-10-08T17:33:55.481487Z"
    },
    "papermill": {
     "duration": 2497.803043,
     "end_time": "2020-12-22T20:26:04.409666",
     "exception": false,
     "start_time": "2020-12-22T19:44:26.606623",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! rm -rf TRAIN_IMAGES\n",
    "! rm -rf VAL_IMAGES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
