{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "864d2584",
   "metadata": {
    "papermill": {
     "duration": 0.005318,
     "end_time": "2022-10-11T15:48:24.299647",
     "exception": false,
     "start_time": "2022-10-11T15:48:24.294329",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# [Rainforest Connecton Species Audio Detection Challenge](https://www.kaggle.com/competitions/rfcx-species-audio-detection/data) was a machine learning Kaggle competition to detect species from their audio recordings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa546f2",
   "metadata": {
    "papermill": {
     "duration": 0.003799,
     "end_time": "2022-10-11T15:48:24.307653",
     "exception": false,
     "start_time": "2022-10-11T15:48:24.303854",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Aim\n",
    "\n",
    "Given audio files that include sounds from numerous species, the objective is to use machine learning to predict species in the audio clip. \n",
    "\n",
    "# Data Description\n",
    "Each audio recording is 5 mins long. There are total of 3958 audio recording files availabe for training. \n",
    "Each single recorded audio contains signals from multiple species which are labelled by time-stamps `t_min` and `t_max`, where `t_min` indicates the start time the a particular species was heard and  `t_max` the time until the species was heard. Also provided are frequency range(`f_min`, `f_max`) of that particular siganl. \n",
    "\n",
    "\n",
    "Training data also includes false positive label occurrences to assist with training.\n",
    "\n",
    "Files: \n",
    "\n",
    "- `train_tp.csv` - training data of true positive species labels, with corresponding time localization.\n",
    "- `train_fp.csv` - training data of false positives species labels, with corresponding time localization\n",
    "- `train/` - the training audio files\n",
    "- `test/` - the test audio files; the task is to predict the species found in each audio file\n",
    "\n",
    "The meta file `train_tp.csv` contains the following columns:\n",
    "\n",
    "- `recording_id` - unique identifier for recording\n",
    "- `species_id` - unique identifier for species\n",
    "- `songtype_id` - unique identifier for songtype\n",
    "- `t_min` - start second of annotated signal\n",
    "- `f_min` - lower frequency of annotated signal\n",
    "- `t_max` - end second of annotated signal\n",
    "- `f_max` - upper frequency of annotated signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4361f7b6",
   "metadata": {
    "papermill": {
     "duration": 0.003786,
     "end_time": "2022-10-11T15:48:24.315482",
     "exception": false,
     "start_time": "2022-10-11T15:48:24.311696",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Approach:\n",
    "\n",
    "Past studies showed that rather than directly use the audio signals for species detection, instead using images of mel-spectrogram of the audio signals can be  used train deep learning models to perform species identification. The approach used in this notebook is the same. \n",
    "\n",
    "\n",
    "Since EDA indicates that each signal is approx. `2.64 sec` long. The plan is to chop each audio recordings (each `5 min` long) into `10 sec` long clips. \n",
    "Then plot the Mel-spectrogram, and save it as images which could be then be used to train deep learning models. There are a total of 24 different species that haven been labelled.  \n",
    "\n",
    "This notebook illustrates how the audio recordings can be preprocessed to generate images of mel-spectrogram, and  how these images can used to detect species using a deep-learning model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8c60dc",
   "metadata": {
    "papermill": {
     "duration": 0.00389,
     "end_time": "2022-10-11T15:48:24.323347",
     "exception": false,
     "start_time": "2022-10-11T15:48:24.319457",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e5ec169",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-10-11T15:48:24.334042Z",
     "iopub.status.busy": "2022-10-11T15:48:24.332847Z",
     "iopub.status.idle": "2022-10-11T15:48:33.068773Z",
     "shell.execute_reply": "2022-10-11T15:48:33.066168Z"
    },
    "papermill": {
     "duration": 8.743759,
     "end_time": "2022-10-11T15:48:33.071114",
     "exception": false,
     "start_time": "2022-10-11T15:48:24.327355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "# Optimizer\n",
    "import torch.optim as optim\n",
    "# Libraries for handling dataset\n",
    "from torch.utils.data import Dataset as BaseDataset, DataLoader\n",
    "\n",
    "# LR Scheduler\n",
    "from transformers import get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup\n",
    "\n",
    "from torchvision import transforms \n",
    "\n",
    "# Load pretrained computer-vision model\n",
    "from torchvision.models.resnet import resnet18, resnet34, resnet50\n",
    "\n",
    "# Libraries for handling audio files\n",
    "import librosa as lb  \n",
    "\n",
    "# Libraries for plotting Spectrograms from audio signals\n",
    "# from audiomentations import *\n",
    "\n",
    "# Libraries for handling images\n",
    "import skimage.io\n",
    "from skimage import transform\n",
    "\n",
    "print(lb.__version__)\n",
    "#print(python.__version__)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af767bf",
   "metadata": {
    "papermill": {
     "duration": 0.004215,
     "end_time": "2022-10-11T15:48:33.079834",
     "exception": false,
     "start_time": "2022-10-11T15:48:33.075619",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Quick EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "687afc70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-11T15:48:33.090139Z",
     "iopub.status.busy": "2022-10-11T15:48:33.089326Z",
     "iopub.status.idle": "2022-10-11T15:48:33.175481Z",
     "shell.execute_reply": "2022-10-11T15:48:33.174375Z"
    },
    "papermill": {
     "duration": 0.093452,
     "end_time": "2022-10-11T15:48:33.177524",
     "exception": false,
     "start_time": "2022-10-11T15:48:33.084072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recording_id</th>\n",
       "      <th>species_id</th>\n",
       "      <th>songtype_id</th>\n",
       "      <th>t_min</th>\n",
       "      <th>f_min</th>\n",
       "      <th>t_max</th>\n",
       "      <th>f_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00204008d</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>13.8400</td>\n",
       "      <td>3281.2500</td>\n",
       "      <td>14.9333</td>\n",
       "      <td>4125.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00204008d</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>24.4960</td>\n",
       "      <td>3750.0000</td>\n",
       "      <td>28.6187</td>\n",
       "      <td>5531.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00204008d</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0027</td>\n",
       "      <td>2343.7500</td>\n",
       "      <td>16.8587</td>\n",
       "      <td>4218.7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003b04435</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>43.2533</td>\n",
       "      <td>10687.5000</td>\n",
       "      <td>44.8587</td>\n",
       "      <td>13687.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003b04435</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>9.1254</td>\n",
       "      <td>7235.1562</td>\n",
       "      <td>15.2091</td>\n",
       "      <td>11283.3984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  recording_id  species_id  songtype_id    t_min       f_min    t_max  \\\n",
       "0    00204008d          21            1  13.8400   3281.2500  14.9333   \n",
       "1    00204008d           8            1  24.4960   3750.0000  28.6187   \n",
       "2    00204008d           4            1  15.0027   2343.7500  16.8587   \n",
       "3    003b04435          22            1  43.2533  10687.5000  44.8587   \n",
       "4    003b04435          23            1   9.1254   7235.1562  15.2091   \n",
       "\n",
       "        f_max  \n",
       "0   4125.0000  \n",
       "1   5531.2500  \n",
       "2   4218.7500  \n",
       "3  13687.5000  \n",
       "4  11283.3984  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00204008d' '003b04435' '005f1f9a5' ... 'ffebe7313' 'fff163132'\n",
      " 'fffb79246'] # Number of recordings =  3958\n",
      "[21  8  4 22 23 10  2  1 11 19 20 17  9 15 16  6  5  3 18 12  0 13 14  7] # Number of species =  24\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>species_id</th>\n",
       "      <td>7781.0</td>\n",
       "      <td>12.138671</td>\n",
       "      <td>7.068808</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>12.0000</td>\n",
       "      <td>18.0000</td>\n",
       "      <td>23.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>songtype_id</th>\n",
       "      <td>7781.0</td>\n",
       "      <td>1.346999</td>\n",
       "      <td>0.959535</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>4.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_min</th>\n",
       "      <td>7781.0</td>\n",
       "      <td>28.627830</td>\n",
       "      <td>17.461603</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>12.9493</td>\n",
       "      <td>28.8800</td>\n",
       "      <td>44.0657</td>\n",
       "      <td>59.3013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_min</th>\n",
       "      <td>7781.0</td>\n",
       "      <td>2827.996428</td>\n",
       "      <td>2515.604420</td>\n",
       "      <td>93.7500</td>\n",
       "      <td>947.4609</td>\n",
       "      <td>2343.7500</td>\n",
       "      <td>3843.7500</td>\n",
       "      <td>10687.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_max</th>\n",
       "      <td>7781.0</td>\n",
       "      <td>31.267911</td>\n",
       "      <td>17.496989</td>\n",
       "      <td>0.7680</td>\n",
       "      <td>15.7280</td>\n",
       "      <td>31.5413</td>\n",
       "      <td>46.7893</td>\n",
       "      <td>59.9947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f_max</th>\n",
       "      <td>7781.0</td>\n",
       "      <td>6074.830415</td>\n",
       "      <td>3386.040304</td>\n",
       "      <td>843.7500</td>\n",
       "      <td>3937.5000</td>\n",
       "      <td>5250.0000</td>\n",
       "      <td>7500.0000</td>\n",
       "      <td>13687.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>signal_duration</th>\n",
       "      <td>7781.0</td>\n",
       "      <td>2.640081</td>\n",
       "      <td>1.820595</td>\n",
       "      <td>0.2720</td>\n",
       "      <td>1.2906</td>\n",
       "      <td>2.1440</td>\n",
       "      <td>3.3440</td>\n",
       "      <td>7.9239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  count         mean          std       min        25%  \\\n",
       "species_id       7781.0    12.138671     7.068808    0.0000     6.0000   \n",
       "songtype_id      7781.0     1.346999     0.959535    1.0000     1.0000   \n",
       "t_min            7781.0    28.627830    17.461603    0.0107    12.9493   \n",
       "f_min            7781.0  2827.996428  2515.604420   93.7500   947.4609   \n",
       "t_max            7781.0    31.267911    17.496989    0.7680    15.7280   \n",
       "f_max            7781.0  6074.830415  3386.040304  843.7500  3937.5000   \n",
       "signal_duration  7781.0     2.640081     1.820595    0.2720     1.2906   \n",
       "\n",
       "                       50%        75%         max  \n",
       "species_id         12.0000    18.0000     23.0000  \n",
       "songtype_id         1.0000     1.0000      4.0000  \n",
       "t_min              28.8800    44.0657     59.3013  \n",
       "f_min            2343.7500  3843.7500  10687.5000  \n",
       "t_max              31.5413    46.7893     59.9947  \n",
       "f_max            5250.0000  7500.0000  13687.5000  \n",
       "signal_duration     2.1440     3.3440      7.9239  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg signal duration (sec) 2.640081403418584\n",
      "Number of species = 24\n"
     ]
    }
   ],
   "source": [
    "# Review rfcx-species-detection challenge \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "path = '../input/rfcx-species-audio-detection'\n",
    "train_csv = pd.read_csv(path + '/train_fp.csv')\n",
    "\n",
    "display(train_csv.head())\n",
    "\n",
    "rec_ids = train_csv.recording_id.unique()\n",
    "print(rec_ids, '# Number of recordings = ', len(rec_ids))\n",
    "\n",
    "species_ids = train_csv.species_id.unique()\n",
    "print(species_ids, '# Number of species = ', len(species_ids))\n",
    "\n",
    "# Compute average duration of signal\n",
    "train_csv['signal_duration'] = train_csv.t_max - train_csv.t_min\n",
    "display(train_csv.describe().transpose())\n",
    "\n",
    "avg_signal_duration = train_csv.signal_duration.mean()\n",
    "print('Avg signal duration (sec)', avg_signal_duration)\n",
    "\n",
    "print('Number of species =', train_csv.species_id.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fa92f1",
   "metadata": {
    "papermill": {
     "duration": 0.004516,
     "end_time": "2022-10-11T15:48:33.186888",
     "exception": false,
     "start_time": "2022-10-11T15:48:33.182372",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# class `Preprocessing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6deec2a4",
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2022-10-11T15:48:33.197609Z",
     "iopub.status.busy": "2022-10-11T15:48:33.197325Z",
     "iopub.status.idle": "2022-10-11T15:48:33.218794Z",
     "shell.execute_reply": "2022-10-11T15:48:33.217910Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.029292,
     "end_time": "2022-10-11T15:48:33.220762",
     "exception": false,
     "start_time": "2022-10-11T15:48:33.191470",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    \n",
    "    sample_length = 5 \n",
    "    sampling_rate = 48000\n",
    "    image_size = (224, 224)\n",
    "    AUDIO_FOLDER = '../input/rfcx-species-audio-detection'\n",
    "   \n",
    "    def __init__(self, meta_data):\n",
    "        self.META_FILE = meta_data\n",
    "        \n",
    "    def train_data(self, OUTPUT_IMAGE_FOLDER = 'TRAIN_IMAGES'):\n",
    "        '''\n",
    "        PREPROCESSING: AUDIO SIGNAL --> IMAGE.\n",
    "        Preprocessing steps: \n",
    "        1. Uses package librosa to load the audio file as a numpy array.\n",
    "        2. Select the sample duration from the audio-file.\n",
    "        3. Plot the Mel-Spectrogram of the sample.\n",
    "        4. Convert the MEL-spectrogram to image of desired size.\n",
    "        5. Save the image.\n",
    "        '''\n",
    "        start_time = time.time()\n",
    "\n",
    "        for idx in range(len(self.META_FILE)):\n",
    "            record_id = self.META_FILE['recording_id'].iloc[idx] \n",
    "            label = self.META_FILE['species_id'].iloc[idx]\n",
    "\n",
    "            # Use librosa to load the audio file as a numpy array...\n",
    "            audio_filepath = f'{self.AUDIO_FOLDER}/train/{record_id}.flac'\n",
    "            \n",
    "            audio_data, _ = lb.load(audio_filepath, sr = self.sampling_rate) # sr -  sampling rate\n",
    "\n",
    "            # Duration of signal is (t_max - t_min). \n",
    "            t_min = self.META_FILE['t_min'].iloc[idx] \n",
    "            t_max = self.META_FILE['t_max'].iloc[idx]\n",
    "\n",
    "            # Select sample start time such that it contains the signal.\n",
    "            sample_start_time =  (t_min + t_max) * 0.5 - self.sample_length * 0.5\n",
    "            sample_end_time = sample_start_time + self.sample_length\n",
    "\n",
    "            if sample_start_time < 0:\n",
    "                sample_start_time = 0.\n",
    "                sample_end_time = sample_start_time + self.sample_length\n",
    "\n",
    "            if sample_end_time > (len(audio_data) / self.sampling_rate):\n",
    "                sample_end_time = len(audio_data) / self.sampling_rate\n",
    "                sample_start_time = sample_end_time - self.sample_length\n",
    "\n",
    "            start_idx = int(sample_start_time * self.sampling_rate)\n",
    "            end_idx = int(sample_end_time * self.sampling_rate)\n",
    "\n",
    "            sample = audio_data[start_idx:end_idx]\n",
    "            S = lb.feature.melspectrogram(sample, self.sampling_rate,  \n",
    "                n_fft = 2048,                   # WINDOW LENGHT             \n",
    "                hop_length = 512,               # WINDOW LENGHT // 4  \n",
    "                n_mels = 128, \n",
    "                fmin = 0,                    \n",
    "                fmax = self.sampling_rate // 2 # HALF THE SAMPLING RATE     \n",
    "            )\n",
    "            # Convert ot power\n",
    "            D = lb.power_to_db(S, ref=np.max)\n",
    "\n",
    "            # Resize the MEL-Spectrogram to image of (224x224)\n",
    "            D = skimage.transform.resize(D, self.image_size)\n",
    "\n",
    "            # Normalize the image\n",
    "            D = D - D.min()\n",
    "            D = D / (D.max() + 1.0e-10)        \n",
    "\n",
    "            # Save image as np.array...\n",
    "            if not Path(OUTPUT_IMAGE_FOLDER).exists():\n",
    "                os.mkdir(OUTPUT_IMAGE_FOLDER)          \n",
    "            path_train_images = Path(OUTPUT_IMAGE_FOLDER) / ('RID{}_SID{:0>2d}'.format(record_id, label))  \n",
    "            np.save(path_train_images, D)\n",
    "            \n",
    "        print('Number of audio files for training:', len(self.META_FILE))\n",
    "        print(f'TRAINING IMAGES SAVED TO FOLDER: {OUTPUT_IMAGE_FOLDER}')\n",
    "        print('Number of training images: ', len(os.listdir(OUTPUT_IMAGE_FOLDER)))\n",
    "        print('Image Size:', self.image_size)\n",
    "        print('Processing time {:.2f} seconds'.format(time.time() - start_time ))\n",
    "\n",
    "    def val_data(self, OUTPUT_IMAGE_FOLDER = 'VAL_IMAGES'):\n",
    "        start_time = time.time()\n",
    "        for idx in range(len(self.META_FILE)):    \n",
    "                record_id = self.META_FILE['recording_id'].iloc[idx] \n",
    "                label = self.META_FILE['species_id'].iloc[idx]\n",
    "                audio_file  = f'{self.AUDIO_FOLDER}/train/{record_id}.flac'\n",
    "                audio_data, _ = lb.load(audio_file, sr = self.sampling_rate) \n",
    "                n_samples = len(audio_data) // (self.sample_length * self.sampling_rate)\n",
    "\n",
    "                start_idx = 0\n",
    "                end_idx = start_idx + (self.sample_length * self.sampling_rate)\n",
    "\n",
    "                if len(audio_data) < self.sample_length * self.sampling_rate:\n",
    "                    n_samples = 1\n",
    "                    end_idx = len(audio_data) \n",
    "\n",
    "                for j in range(n_samples):\n",
    "                    S = lb.feature.melspectrogram(audio_data[start_idx:end_idx], self.sampling_rate, n_fft = 2048, hop_length = 2048 // 4, n_mels = 128,\n",
    "                                                  fmin = 0, fmax = self.sampling_rate // 2)\n",
    "                    D = lb.power_to_db(S, ref= np.max)\n",
    "                    # RESIZE\n",
    "                    D = skimage.transform.resize(D, (224, 224))\n",
    "                    # Normalize\n",
    "                    D = D - D.min()\n",
    "                    D = D / (D.max() + 1.0e-10)   \n",
    "                    if not Path(OUTPUT_IMAGE_FOLDER).exists():\n",
    "                        os.mkdir(OUTPUT_IMAGE_FOLDER)      \n",
    "                    path_val_images = Path(OUTPUT_IMAGE_FOLDER) / ('RID{}_SID{:0>2d}_CLIP{:0>2d}'.format(record_id, label, j))  \n",
    "                    np.save(path_val_images, D)\n",
    "                    # Update the `start_idx` and `end_idx`\n",
    "                    start_idx = end_idx\n",
    "                    end_idx = start_idx + self.sample_length * self.sampling_rate\n",
    "        print('') \n",
    "        print('Number of audio files for validation:', len(self.META_FILE))\n",
    "        print(f'VALIDATION IMAGES SAVED TO FOLDER: {OUTPUT_IMAGE_FOLDER}')\n",
    "        print('Number of validating images: ', len(os.listdir(OUTPUT_IMAGE_FOLDER)))\n",
    "        print('Image Size:', self.image_size)\n",
    "        print('Processing time {:.2f} seconds'.format(time.time() - start_time ))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094a335a",
   "metadata": {
    "papermill": {
     "duration": 0.004508,
     "end_time": "2022-10-11T15:48:33.229877",
     "exception": false,
     "start_time": "2022-10-11T15:48:33.225369",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Metric: label-weighted label rankded average precision `(lwlrap)` \n",
    "1. https://scikit-learn.org/stable/modules/model_evaluation.html#label-ranking-average-precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7feaaa3",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-10-11T15:48:33.240434Z",
     "iopub.status.busy": "2022-10-11T15:48:33.240158Z",
     "iopub.status.idle": "2022-10-11T15:48:33.250836Z",
     "shell.execute_reply": "2022-10-11T15:48:33.249942Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.018557,
     "end_time": "2022-10-11T15:48:33.253016",
     "exception": false,
     "start_time": "2022-10-11T15:48:33.234459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _one_sample_positive_class_precisions(scores, truth, TRAIN, threshold):\n",
    "    num_classes = scores.shape[0]\n",
    "    \n",
    "    if not TRAIN: threshold = 0\n",
    "    pos_class_indices = np.flatnonzero(truth > threshold)\n",
    "    if not len(pos_class_indices):\n",
    "        return pos_class_indices, np.zeros(0)\n",
    "\n",
    "    retrieved_classes = np.argsort(scores)[::-1]\n",
    "    class_rankings = np.zeros(num_classes, dtype=np.int32)\n",
    "    class_rankings[retrieved_classes] = range(num_classes)\n",
    "    retrieved_class_true = np.zeros(num_classes, dtype=np.bool)\n",
    "    retrieved_class_true[class_rankings[pos_class_indices]] = True\n",
    "    retrieved_cumulative_hits = np.cumsum(retrieved_class_true)\n",
    "    precision_at_hits = (\n",
    "            retrieved_cumulative_hits[class_rankings[pos_class_indices]] /\n",
    "            (1 + class_rankings[pos_class_indices].astype(np.float32)))\n",
    "    return pos_class_indices, precision_at_hits\n",
    "\n",
    "def lwlrap(truth, scores, ALPHA_LABEL = 0, TRAIN=True):\n",
    "    \"\"\" \n",
    "    Label-Weighted Label Rated Average Precision.\n",
    "    Reference:\n",
    "    1. https://www.kaggle.com/c/rfcx-species-audio-detection/discussion/198418\n",
    "    2. https://scikit-learn.org/stable/modules/model_evaluation.html#label-ranking-average-precision\n",
    "    \"\"\"\n",
    "    assert truth.shape == scores.shape\n",
    "    \n",
    "    num_samples, num_classes = scores.shape\n",
    "\n",
    "    if TRAIN:\n",
    "        threshold = (ALPHA_LABEL / num_classes)\n",
    "    else:\n",
    "        threshold = 0\n",
    "    \n",
    "    precisions_for_samples_by_classes = np.zeros((num_samples, num_classes), dtype=np.float32)\n",
    "    \n",
    "    for sample_num in range(num_samples):\n",
    "        pos_class_indices, precision_at_hits = _one_sample_positive_class_precisions(\n",
    "            scores[sample_num, :], truth[sample_num, :], TRAIN, threshold\n",
    "        )   \n",
    "        precisions_for_samples_by_classes[sample_num, pos_class_indices] = precision_at_hits\n",
    "    \n",
    "    \n",
    "    #labels_per_class = np.sum(truth > 0, axis=0)\n",
    "    labels_per_class = np.sum(truth > threshold, axis=0)\n",
    "\n",
    "    #weight_per_class = labels_per_class / float(np.sum(labels_per_class))\n",
    "    weight_per_class = labels_per_class / np.sum(labels_per_class).astype(np.float32)\n",
    "\n",
    "    per_class_lwlrap = (np.sum(precisions_for_samples_by_classes, axis=0) /\n",
    "                np.maximum(1, labels_per_class))\n",
    "    \n",
    "    return per_class_lwlrap, weight_per_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aee95d",
   "metadata": {
    "papermill": {
     "duration": 0.004576,
     "end_time": "2022-10-11T15:48:33.262072",
     "exception": false,
     "start_time": "2022-10-11T15:48:33.257496",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# `class Train_Validate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15cbbb0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-11T15:48:33.272919Z",
     "iopub.status.busy": "2022-10-11T15:48:33.272656Z",
     "iopub.status.idle": "2022-10-11T15:48:33.374075Z",
     "shell.execute_reply": "2022-10-11T15:48:33.373056Z"
    },
    "papermill": {
     "duration": 0.109203,
     "end_time": "2022-10-11T15:48:33.376022",
     "exception": false,
     "start_time": "2022-10-11T15:48:33.266819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Train_Validate(nn.Module):\n",
    "    \n",
    "    device =  torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # Training Parameters...\n",
    "    CHECKPOINT_EPOCH = 5\n",
    "    EPOCHS = 5\n",
    "    LR = 1.0e-03             # Initial Learing Rate\n",
    "    MODEL_NAME = 'resnet34'\n",
    "    MODEL_STATE_FOLDER = 'MODEL-STATE'\n",
    "    NUM_CLASSES = 24    \n",
    "    # Cosine Scheduler with Warmup start\n",
    "    num_warmup_steps = 5\n",
    "    num_training_steps = 30 \n",
    "\n",
    "        \n",
    "    def __init__(self, fold=0):\n",
    "        super().__init__()\n",
    "        self.fold = fold\n",
    "        self.model = self.get_model(self.MODEL_NAME)\n",
    "        self.model.to(self.device)\n",
    "          \n",
    "        self.criterion = nn.BCEWithLogitsLoss().to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr = self.LR)  \n",
    "        self.scheduler = get_cosine_schedule_with_warmup(self.optimizer, self.num_warmup_steps, self.num_training_steps)\n",
    "        \n",
    "    def get_model(self, MODEL_NAME, PRETRAINED=True):\n",
    "        if MODEL_NAME == 'resnet34':\n",
    "            model = resnet34(pretrained=PRETRAINED)  \n",
    "            del model.fc\n",
    "            model.fc = nn.Linear(in_features=512, out_features=self.NUM_CLASSES, bias=True)\n",
    "        return model\n",
    "\n",
    "    def fit(self, train_dl, val_dl):\n",
    "        total_train_loss = []\n",
    "        total_val_loss = []\n",
    "        total_train_score = []\n",
    "        total_val_score = []\n",
    "        for epoch in range(self.EPOCHS):\n",
    "            train_loss, train_score = self.train_fn(train_dl)            \n",
    "            total_train_loss.append(train_loss)\n",
    "            total_train_score.append(train_score)\n",
    "            #print('TRAIN: Epoch: {:0>2d};  Loss = {:.3f}; Precision = {:.3f}'.format(epoch, train_loss, train_score))\n",
    "            val_loss, val_score = self.val_fn(val_dl)\n",
    "            total_val_loss.append(val_loss)\n",
    "            total_val_score.append(val_score)  \n",
    "            print('Epoch: {:0>2d}; Train: Loss = {:.3f}; Precision = {:.3f}; Val: Loss = {:.3f}; Precision = {:.3f}'.format(\n",
    "                epoch, train_loss, train_score, val_loss, val_score))            \n",
    "            # Call the step fn on the scheduler to update the lr...\n",
    "            self.scheduler.step()\n",
    "        # Plot the results...\n",
    "        self.visualize_results(total_train_loss, total_train_score, total_val_loss, total_val_score)\n",
    "            \n",
    "    def train_fn(self, train_dl):\n",
    "        self.model.train()\n",
    "        losses = []\n",
    "        outputs = []\n",
    "        labels = []\n",
    "        for input, label in train_dl:\n",
    "            output = self.model(input.to(self.device))                  \n",
    "            loss = self.criterion(output, label.to(self.device))\n",
    "            self.optimizer.zero_grad()               \n",
    "            loss.backward()                \n",
    "            self.optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "            # Saving model ouput and labels to check Precision.....\n",
    "            outputs.append(torch.sigmoid(output).cpu().detach().numpy()) \n",
    "            labels.append(label.cpu().detach().numpy())  \n",
    "        # Compute Precision ...\n",
    "        score = self.get_precision(labels, outputs)\n",
    "        return np.mean(losses), score\n",
    "    \n",
    "    def val_fn(self, val_dl):\n",
    "        self.model.eval()\n",
    "        labels = []\n",
    "        probs = []\n",
    "        losses = []\n",
    "        with torch.no_grad():\n",
    "            for image, label in val_dl:\n",
    "                # Make prediction...\n",
    "                pred = self.model(image.to(self.device))\n",
    "                loss = self.criterion(pred, label.to(self.device))\n",
    "                losses.append(loss.item())\n",
    "                # Get probabilities...\n",
    "                prob = torch.sigmoid(pred)\n",
    "                max_prob, _ = torch.max(prob, axis = 0)                    \n",
    "                probs.append(max_prob.cpu().detach().numpy())\n",
    "                labels.append(label[0, :].cpu().detach().numpy())   \n",
    "        # Compute Precision ...        \n",
    "        score = self.get_precision(labels, probs)\n",
    "        return np.mean(losses), score\n",
    "    \n",
    "    def get_precision(self, labels, probs, TRAIN=True):\n",
    "        y_score = np.vstack(probs)\n",
    "        y_true = np.vstack(labels)\n",
    "        score_class, weight = lwlrap(y_true, y_score, TRAIN)\n",
    "        score_v = (score_class * weight).sum()\n",
    "        return score_v\n",
    "    \n",
    "    def model_checkpoint(self, epoch):\n",
    "        if (epoch+1) % self.CHECKPOINT_EPOCH == 0:\n",
    "            if not Path(self.MODEL_STATE_FOLDER).exists():\n",
    "                os.mkdir(self.MODEL_STATE_FOLDER)\n",
    "            path_model_checkpoint = Path(self.MODEL_STATE_FOLDER) / 'rfcx-{}-EPOCH{}-FOLD{}'.format(MODEL_NAME, epoch+1, self.fold)\n",
    "            torch.save(\n",
    "                {\n",
    "                'spectrogram_params': params,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'opt_state_dict': optimizer.state_dict(),\n",
    "                'epoch': (epoch+1),\n",
    "                'loss': loss\n",
    "                }, \n",
    "                path_model_checkpoint\n",
    "            )   \n",
    "            \n",
    "    def visualize_results(self, train_loss, train_score, val_loss, val_score):\n",
    "        fig, axes = plt.subplots(1, 2, sharex=True, figsize=(6, 4))\n",
    "        ax = np.ravel(axes)\n",
    "        ax[0].plot(train_loss, 'r-', val_loss, 'b-')\n",
    "        ax[0].set_ylim([0, 1.])\n",
    "        ax[0].set_ylabel('Loss')\n",
    "        ax[0].legend(['train', 'val'])\n",
    "        ax[1].plot(train_score, 'r-', val_score, 'b-')\n",
    "        ax[1].set_ylim([0, 1.])    \n",
    "        ax[1].set_ylabel('Precision')\n",
    "        ax[1].set_xlabel('Epoch')\n",
    "        ax[1].legend(['train', 'val'])\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebf3417",
   "metadata": {
    "papermill": {
     "duration": 0.004342,
     "end_time": "2022-10-11T15:48:33.385055",
     "exception": false,
     "start_time": "2022-10-11T15:48:33.380713",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0afd445",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-10-11T15:48:33.396858Z",
     "iopub.status.busy": "2022-10-11T15:48:33.396153Z",
     "iopub.status.idle": "2022-10-11T15:48:33.405755Z",
     "shell.execute_reply": "2022-10-11T15:48:33.404845Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.01725,
     "end_time": "2022-10-11T15:48:33.407795",
     "exception": false,
     "start_time": "2022-10-11T15:48:33.390545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dataset(BaseDataset):\n",
    "    def __init__(self, TRAIN=True):\n",
    "        super().__init__()\n",
    "        self.train = TRAIN\n",
    "        if self.train:\n",
    "            self.path_images = 'TRAIN_IMAGES'\n",
    "            self.images = sorted(os.listdir(self.path_images))\n",
    "        else:\n",
    "            self.path_images = 'VAL_IMAGES'\n",
    "            self.images = sorted(os.listdir(self.path_images))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images) \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        if self.train:\n",
    "            if len(image) <= 22:\n",
    "                label = int(image[16:18])\n",
    "                onehot_label = np.zeros(24, dtype = np.float32)\n",
    "                onehot_label[label] = 1.0\n",
    "            else:\n",
    "                label1 = int(file[16:18])\n",
    "                label2 = int(file[18:20])\n",
    "                label3 = int(file[20:22])\n",
    "\n",
    "                onehot_label = np.zeros(24, dtype = np.float32)\n",
    "                onehot_label[label1] = 1.0\n",
    "                onehot_label[label2] = 1.0\n",
    "                onehot_label[label3] = 1.0\n",
    "        else:\n",
    "            label = int(image[16:18])\n",
    "            onehot_label = np.zeros(24, dtype = np.float32)\n",
    "            onehot_label[label] = 1.0\n",
    "\n",
    "        # Load the spectrogram from the folder...\n",
    "        X = np.load(self.path_images + '/' + image)             \n",
    "        # Convert mono to color with first dimension as channel.\n",
    "        image_3d = np.stack([X, X, X]) #, dtype=np.float32)         \n",
    "        return image_3d.astype(np.float32), onehot_label\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03547cbb",
   "metadata": {
    "papermill": {
     "duration": 0.004277,
     "end_time": "2022-10-11T15:48:33.416533",
     "exception": false,
     "start_time": "2022-10-11T15:48:33.412256",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Split data into 80\\% `training` and 20% `validation` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fffd278",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-10-11T15:48:33.427118Z",
     "iopub.status.busy": "2022-10-11T15:48:33.426348Z",
     "iopub.status.idle": "2022-10-11T15:52:56.588188Z",
     "shell.execute_reply": "2022-10-11T15:52:56.586883Z"
    },
    "papermill": {
     "duration": 263.169551,
     "end_time": "2022-10-11T15:52:56.590561",
     "exception": false,
     "start_time": "2022-10-11T15:48:33.421010",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD = 0\n",
      "Number of audio files for training: 800\n",
      "TRAINING IMAGES SAVED TO FOLDER: TRAIN_IMAGES\n",
      "Number of training images:  773\n",
      "Image Size: (224, 224)\n",
      "Processing time 113.70 seconds\n",
      "\n",
      "Number of audio files for validation: 200\n",
      "VALIDATION IMAGES SAVED TO FOLDER: VAL_IMAGES\n",
      "Number of validating images:  2388\n",
      "Image Size: (224, 224)\n",
      "Processing time 97.78 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ccc3ec5b6894470a49c56f5a94d240c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/83.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00; Train: Loss = 0.698; Precision = 0.146; Val: Loss = 0.714; Precision = 0.161\n",
      "Epoch: 01; Train: Loss = 0.247; Precision = 0.466; Val: Loss = 0.139; Precision = 0.573\n",
      "Epoch: 02; Train: Loss = 0.086; Precision = 0.820; Val: Loss = 0.160; Precision = 0.495\n",
      "Epoch: 03; Train: Loss = 0.059; Precision = 0.877; Val: Loss = 0.160; Precision = 0.500\n",
      "Epoch: 04; Train: Loss = 0.053; Precision = 0.881; Val: Loss = 0.155; Precision = 0.554\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD = 1\n",
      "FOLD = 2\n",
      "FOLD = 3\n",
      "FOLD = 4\n"
     ]
    }
   ],
   "source": [
    "AUDIO_FOLDER = '../input/rfcx-species-audio-detection'\n",
    "\n",
    "# Load the meta-file\n",
    "META_FILE = pd.read_csv(f'{AUDIO_FOLDER}/train_tp.csv')\n",
    "\n",
    "# We only use with a small fraction of the available data (1000 audio recordings) for now ...\n",
    "META_FILE = META_FILE.iloc[:1000]\n",
    "\n",
    "# Split the data into train and validation using StratifiedKFold...\n",
    "from sklearn.model_selection import StratifiedKFold as skf\n",
    "kfold = skf(n_splits=5, shuffle=True, random_state=32)\n",
    "record_ids = META_FILE['recording_id'].tolist()\n",
    "species_ids = META_FILE['species_id'].tolist()\n",
    "\n",
    "for fold, (train_indices, val_indices) in enumerate(\n",
    "    kfold.split(record_ids, species_ids)\n",
    "):        \n",
    "    print('FOLD =', fold)\n",
    "    # Only using fold 0...\n",
    "    if fold == 0:\n",
    "        # Load Training dataset...\n",
    "        TRAIN_META_FILE = META_FILE.iloc[train_indices] \n",
    "        Preprocessing(TRAIN_META_FILE).train_data()\n",
    "        train_ds = Dataset(TRAIN=True)\n",
    "        train_dl = DataLoader(train_ds, batch_size = 16, shuffle = True, num_workers = 0)\n",
    "\n",
    "        # Load Validation data ...\n",
    "        VAL_META_FILE = META_FILE.iloc[val_indices]\n",
    "        Preprocessing(VAL_META_FILE).val_data()                       \n",
    "        val_ds = Dataset(TRAIN=False) \n",
    "        val_dl = DataLoader(val_ds, batch_size=8, shuffle = False, drop_last = True)\n",
    "                \n",
    "        # Perform training and validation...\n",
    "        Train_Validate(fold).fit(train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd43d950",
   "metadata": {
    "papermill": {
     "duration": 0.00541,
     "end_time": "2022-10-11T15:52:56.601891",
     "exception": false,
     "start_time": "2022-10-11T15:52:56.596481",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Concluding Remarks\n",
    "\n",
    "Further improvement in the result can be achieved by adding 1. `image augmentations` 2. `mixup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5b7e172",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-10-11T15:52:56.615642Z",
     "iopub.status.busy": "2022-10-11T15:52:56.614184Z",
     "iopub.status.idle": "2022-10-11T15:52:58.770758Z",
     "shell.execute_reply": "2022-10-11T15:52:58.769235Z"
    },
    "papermill": {
     "duration": 2.166234,
     "end_time": "2022-10-11T15:52:58.773645",
     "exception": false,
     "start_time": "2022-10-11T15:52:56.607411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! rm -rf TRAIN_IMAGES\n",
    "! rm -rf VAL_IMAGES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3c217a",
   "metadata": {
    "papermill": {
     "duration": 0.006145,
     "end_time": "2022-10-11T15:52:58.786473",
     "exception": false,
     "start_time": "2022-10-11T15:52:58.780328",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 285.461917,
   "end_time": "2022-10-11T15:53:01.812413",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-10-11T15:48:16.350496",
   "version": "2.3.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1409e9eb13fa451b8ee258ee128be5a1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2aaf82c1875d4d8a9653c09cf6cabbdd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1409e9eb13fa451b8ee258ee128be5a1",
       "placeholder": "",
       "style": "IPY_MODEL_56f21c76f18f41d4bcdfb4a1204921d6",
       "value": " 83.3M/83.3M [00:01&lt;00:00, 54.7MB/s]"
      }
     },
     "56f21c76f18f41d4bcdfb4a1204921d6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6898f25dcbb84460b32a0db0a2f2f101": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e8d16f6625ea4516a47e0752ca5d107f",
       "max": 87319819.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b74c7a64088848b1bb33c8f323db3c84",
       "value": 87319819.0
      }
     },
     "9ccc3ec5b6894470a49c56f5a94d240c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d2456bc3068d42b28ab4ca2456aed675",
        "IPY_MODEL_6898f25dcbb84460b32a0db0a2f2f101",
        "IPY_MODEL_2aaf82c1875d4d8a9653c09cf6cabbdd"
       ],
       "layout": "IPY_MODEL_dda6128de4ac40e3994ebc7fb8493107"
      }
     },
     "b74c7a64088848b1bb33c8f323db3c84": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "cc5388a9aa8f4eb9ba0d3a959d057eb7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "d2456bc3068d42b28ab4ca2456aed675": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d30bf5e1d63042e58bbcb2ccbf278a29",
       "placeholder": "",
       "style": "IPY_MODEL_cc5388a9aa8f4eb9ba0d3a959d057eb7",
       "value": "100%"
      }
     },
     "d30bf5e1d63042e58bbcb2ccbf278a29": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dda6128de4ac40e3994ebc7fb8493107": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e8d16f6625ea4516a47e0752ca5d107f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
